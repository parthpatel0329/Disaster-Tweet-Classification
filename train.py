# -*- coding: utf-8 -*-
"""train.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ytrCjq5xR79xMdGAu69WVozzj4oMvjgV
"""

import pandas as pd
import numpy as np
import torch
import gensim.downloader as api
import nltk
from nltk.stem import WordNetLemmatizer
from nltk.corpus import stopwords
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, f1_score
from sklearn.metrics import classification_report
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.svm import LinearSVC

# Device configuration
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

words = api.load("glove-wiki-gigaword-50")
nltk.download('wordnet')

# import os
# os.chdir('/content/drive/MyDrive/NLP/Natural-Language-Processing-with-Disaster-Tweets-master/Codes')

data = pd.read_csv('data.csv')

tokenizer = nltk.RegexpTokenizer(r"\w+")
lemmatizer = WordNetLemmatizer()

def make_tokens(s):
    s = s.lower()
    tokens = tokenizer.tokenize(s)
    tokens = [lemmatizer.lemmatize(t) for t in tokens]
    return [t for t in tokens if t in words]

def wordVectors(s):
    lis = make_tokens(s)
    v = []

    for token in lis:
        
        if token not in words:
            continue
    
        if token not in t:
            t[token] = ind[0]
            w.append(words[token])
            ind[0] += 1
        
        v.append(np.array(t[token]).reshape(1))
        
    return np.array(v, dtype=float)

def padding(df, colName):
    sequences = []
    
    for i in range(df.shape[0]):
        seq = wordVectors(df[colName][i])
        
        if seq.shape[0] == 0:
            seq = np.zeros(shape=(1, 1))
        
        sequences.append(seq)
    
    cpy = [arr.copy() for arr in sequences]
    
    for i in range(len(sequences)):
        cpy[i] = np.concatenate([sequences[i], np.zeros(shape=(35-sequences[i].shape[0], 1))])
        
    return np.array(cpy).astype(int)

def makeDf(df,colName):
    return padding(df, colName)

t = {}
w = [np.zeros(50).astype(float)]
ind = [1]

X = makeDf(data.copy(),'tweet')
y = data['label'].to_numpy()

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=42)

w = np.stack(w, axis=0)
w = torch.from_numpy(w).float()

class RNN(torch.nn.Module):
    def __init__(self, input_size, hidden_size, num_layers, num_classes, emb):
        super(RNN, self).__init__()
        self.num_layers = num_layers
        self.hidden_size = hidden_size
        self.rnn = torch.nn.RNN(input_size, hidden_size, num_layers, batch_first=True)
        self.fc = torch.nn.Linear(hidden_size, num_classes)
        self.embed = torch.nn.Embedding.from_pretrained(emb)
        
    def forward(self, x):
        x = self.embed(x)[:,0,:,:]
        x = torch.transpose(x, 2, 1)
        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device)
        out, _ = self.rnn(x, h0)
        out = out.mean(dim=1)
        out = self.fc(out)
        out = torch.squeeze(out,1)
        return out

class LSTM(torch.nn.Module):
    def __init__(self, input_size, hidden_size, num_layers, num_classes, emb, bidirectional=False):
        super(LSTM, self).__init__()
        self.num_layers = num_layers
        self.hidden_size = hidden_size
        self.bidirectional = bidirectional
        self.lstm = torch.nn.LSTM(input_size, hidden_size, num_layers, batch_first=True, bidirectional=self.bidirectional)
        
        self.fc = None
        if self.bidirectional:
            self.fc = torch.nn.Linear(hidden_size*2, num_classes)
        else:
            self.fc = torch.nn.Linear(hidden_size, num_classes)
        
        self.embed = torch.nn.Embedding.from_pretrained(emb)
        
    def forward(self, x):
        x = self.embed(x)[:,0,:,:]
        x = torch.transpose(x, 2, 1)
        h0 = None
        c0 = None
        
        if self.bidirectional:
            h0 = torch.zeros(self.num_layers*2, x.size(0), self.hidden_size).to(device)
            c0 = torch.zeros(self.num_layers*2, x.size(0), self.hidden_size).to(device)
        else:
            h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device)
            c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device)
        
        out, _ = self.lstm(x, (h0,c0))
        out = out.mean(dim=1)
        out = self.fc(out)
        out = torch.squeeze(out,1)
        return out

class DAN(torch.nn.Module):
    def __init__(self, input_size, hidden_size, num_classes, emb):
        super(DAN, self).__init__()
        self.layers = torch.nn.Sequential(torch.nn.Linear(input_size, hidden_size),
                                          torch.nn.ReLU(),
                                          torch.nn.Linear(hidden_size, num_classes))
        self.embed = torch.nn.Embedding.from_pretrained(emb)
    def forward(self, x):
        x = self.embed(x)[:,0,:,:]
        x = torch.transpose(x, 2, 1)
        x = x.mean(dim=1)
        x = self.layers(x)
        x = torch.squeeze(x,1)
        return x

#fixed
num_classes = 1
num_layers = 1
batch_size = 30
input_size = 35
sequence_length = 1

train_dataset = []
for i in range(len(X_train)):
    train_dataset.append([torch.from_numpy(X_train[i]), y_train[i]])
    
test_dataset = []
for i in range(len(X_test)):
    test_dataset.append(torch.from_numpy(X_test[i]))

train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=batch_size)
test_loader = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=batch_size)

dan_params = {'num_epochs':150, 'learning_rate':0.1, 'hidden_size':50}
rnn_params = {'num_epochs':25, 'learning_rate':0.1, 'hidden_size':50}
lstm_params = {'num_epochs':100, 'learning_rate':0.1, 'hidden_size':50}
bilstm_params = {'num_epochs':20, 'learning_rate':1, 'hidden_size':50}

for mod in ['RNN','LSTM','Bi-LSTM','DAN']:
    print(mod)
    model = None
    if mod == 'RNN':
        num_epochs = rnn_params['num_epochs']
        learning_rate = rnn_params['learning_rate']
        hidden_size = rnn_params['hidden_size']
        model = RNN(input_size, hidden_size, num_layers, num_classes, w).to(device)
    if mod == 'LSTM':
        num_epochs = lstm_params['num_epochs']
        learning_rate = lstm_params['learning_rate']
        hidden_size = lstm_params['hidden_size']
        model = LSTM(input_size, hidden_size, num_layers, num_classes, w, bidirectional=False).to(device)
    if mod == 'Bi-LSTM':
        num_epochs = bilstm_params['num_epochs']
        learning_rate = bilstm_params['learning_rate']
        hidden_size = bilstm_params['hidden_size']
        model = LSTM(input_size, hidden_size, num_layers, num_classes, w, bidirectional=True).to(device)
    if mod == 'DAN':
        num_epochs = dan_params['num_epochs']
        learning_rate = dan_params['learning_rate']
        hidden_size = dan_params['hidden_size']
        model = DAN(input_size, hidden_size, num_classes, w).to(device)
    criterion = torch.nn.BCEWithLogitsLoss()
    optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)
    loss_values = []
    n_total_steps = len(train_loader)
    for epoch in range(num_epochs):
        running_loss = 0.0
        for i, (inp,labels) in enumerate(train_loader):
            inp = inp.reshape(-1, sequence_length, input_size).to(device)
            labels = labels.to(device)
          
            outputs = model(inp)
            loss = criterion(outputs, labels.float())
          
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()
          
          #if (i+1) % 50 == 0:
          #    print (f'Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{n_total_steps}], Loss: {loss.item():.4f}')
          
            running_loss += loss.item() * inp.size(0)
          
        loss_values.append(running_loss / len(train_dataset))

    y_pred = []
    y_true = y_train.tolist()

    with torch.no_grad():
        for (inp,labels) in train_loader:
            inp = inp.reshape(-1, sequence_length, input_size).to(device)
              
            outputs = model(inp)
            predicted = torch.where(outputs >= 0.0, 1, 0)
            y_pred.extend(predicted.tolist())

    print('Train Accuracy:', accuracy_score(y_train, y_pred))

    y_pred = []
    y_true = y_test.tolist()

    with torch.no_grad():
        for (inp) in test_loader:
            inp = inp.reshape(-1, sequence_length, input_size).to(device)
              
            outputs = model(inp)
            predicted = torch.where(outputs >= 0.0, 1, 0)
            y_pred.extend(predicted.tolist())

    print('Test Accuracy:', accuracy_score(y_test, y_pred))
    print('Classification Report:')
    print(classification_report(y_test, y_pred))

#SVM
print('SVM:')
tfidf = TfidfVectorizer()
classifier = LinearSVC(tol=0.01,random_state = 42)
X = data['tweet']
y = data['label']
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 42)
tfidf.fit(X)
X_train = tfidf.transform(X_train)
X_test = tfidf.transform(X_test)
classifier.fit(X_train,y_train )
y_pred = classifier.predict(X_train)
print('Train Accuracy:', accuracy_score(y_train, y_pred))
y_pred = classifier.predict(X_test)
print('Test Accuracy:', accuracy_score(y_test, y_pred))
print('Classification Report:')
print(classification_report(y_test, y_pred))

